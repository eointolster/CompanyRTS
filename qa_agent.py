# # qa_agent.py

# --- Standard Imports ---
import json
import logging
import asyncio
import zipfile
import re
from pathlib import Path
from datetime import datetime
import traceback
import os
import time
import sys

# --- Import the CLASS from the module ---
from llm_service import LLMService # <-- CHANGE HERE

def perform_qa_work(qa_agent_id, qa_agent_dir, developer_agent_id, developer_agent_dir_path_str):
    """
    Performs QA review on files generated by a developer agent.
    Args:
        qa_agent_id (str): ID of the QA agent performing the work
        qa_agent_dir (Path): Path object pointing to the QA agent's directory
        developer_agent_id (str): ID of the developer agent whose work is being reviewed
        developer_agent_dir_path_str (str): String representation of path to the developer agent's directory
    Returns:
        str: Path to the final zip file containing reviewed files
    """
    # Initialize paths and directories
    developer_agent_dir = Path(developer_agent_dir_path_str)
    qa_logs_dir = qa_agent_dir / "logs"
    developer_files_dir = developer_agent_dir / "files"
    developer_task_file = developer_agent_dir / "task.json"
    qa_agent_info_file = qa_agent_dir / "agent_info.json"

    # Create a final zips directory (assuming BASE_OUTPUT is defined appropriately where app runs)
    # If running this standalone, adjust the base path. For now, use relative "output".
    final_zips_dir = Path("output") / "final_zips"
    qa_logs_dir.mkdir(parents=True, exist_ok=True)
    final_zips_dir.mkdir(parents=True, exist_ok=True)

    # --- Instantiate the LLM Service ---
    llm_service_instance = LLMService() # <-- ADD THIS INSTANTIATION
    # --- ---

    logging.info(f"[{qa_agent_id}] Starting QA work for Developer {developer_agent_id}")

    try:
        # Step 1: Load Developer Files
        project_files = {}
        binary_files = []

        if not developer_files_dir.exists():
            logging.error(f"[{qa_agent_id}] Developer files directory not found: {developer_files_dir}")
            error_report = f"Error: Developer files directory not found at {developer_files_dir}"
            # Create error report file
            error_file_path = qa_logs_dir / f"qa_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(error_file_path, "w", encoding="utf-8") as f:
                f.write(error_report)

            # Create an error zip file
            zip_filename = f"qa_{qa_agent_id}_dev_{developer_agent_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_error.zip"
            zip_filepath = final_zips_dir / zip_filename
            with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(error_file_path, arcname=f"error_report.txt")

            # Update QA agent state
            if qa_agent_info_file.exists():
                with open(qa_agent_info_file, "r+") as f:
                    qa_agent_data = json.load(f)
                    qa_agent_data["state"] = "error"
                    qa_agent_data["error_details"] = error_report
                    qa_agent_data["output_zip_path"] = str(zip_filepath)
                    f.seek(0)
                    json.dump(qa_agent_data, f, indent=2)
                    f.truncate()

            return str(zip_filepath) # Exit early

        # Load all files from developer directory
        text_file_count = 0
        binary_file_count = 0

        # Known binary file extensions
        binary_extensions = {
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.ico', '.svg',
            '.mp3', '.wav', '.mp4', '.avi', '.mov', '.flv', '.wmv',
            '.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx',
            '.zip', '.tar', '.gz', '.7z', '.rar',
            '.exe', '.dll', '.so', '.pyc', '.class'
        }

        for item in developer_files_dir.rglob('*'):
            if item.is_file():
                relative_path = item.relative_to(developer_files_dir)
                file_path_str = str(relative_path)

                # Check if likely binary based on extension
                is_binary = item.suffix.lower() in binary_extensions

                if not is_binary:
                    # Try to read as text first
                    try:
                        with open(item, "r", encoding="utf-8", errors="replace") as f:
                            content = f.read()

                            # Check if content appears to be binary despite extension
                            if '\0' in content or sum(1 for c in content if ord(c) < 32 and c not in '\n\r\t\f\v') > len(content) * 0.3:
                                binary_files.append(file_path_str)
                                binary_file_count += 1
                                continue # Treat as binary

                            project_files[file_path_str] = content
                            text_file_count += 1
                    except Exception as e:
                        logging.warning(f"[{qa_agent_id}] Error reading file {item}: {str(e)}")
                        binary_files.append(file_path_str)
                        binary_file_count += 1
                else:
                    binary_files.append(file_path_str)
                    binary_file_count += 1

        logging.info(f"[{qa_agent_id}] Loaded {text_file_count} text files and identified {binary_file_count} binary files")

        # Step 2: Load Original Task
        task_title = "Unknown Task"
        task_description = "No description available"

        if developer_task_file.exists():
            try:
                with open(developer_task_file, "r", encoding="utf-8") as f:
                    task_data = json.load(f)
                    task_title = task_data.get("title", task_title)
                    task_description = task_data.get("description", task_description)
                logging.info(f"[{qa_agent_id}] Loaded task: {task_title}")
            except Exception as e:
                logging.warning(f"[{qa_agent_id}] Error loading task file: {str(e)}")
        else:
            logging.warning(f"[{qa_agent_id}] Task file not found: {developer_task_file}")

        # Step 3: Prepare LLM Prompt
        combined_files = ""
        for filepath, content in project_files.items():
            combined_files += f"--- START {filepath} ---\n{content}\n--- END {filepath} ---\n\n"

        binary_files_info = ""
        if binary_files:
            binary_files_info = "Binary files (not included in review):\n" + "\n".join(binary_files)

        # Construct the prompt (same as before)
        qa_prompt = f"""
        You are a meticulous QA Engineer reviewing a software project.

        ORIGINAL TASK:
        Title: {task_title}
        Description: {task_description}

        PROJECT FILES:
        The following text contains the full code for all text-based project files.

        {combined_files}

        {binary_files_info}

        YOUR TASK:
        Your goal is to identify and fix errors ONLY. Do NOT add new features, refactor for style (unless it fixes a bug),
        or remove existing functionality. Focus specifically on:

        1. Correctness: Syntax errors, logical flaws, off-by-one errors, incorrect calculations.
        2. Integration: Broken links/paths between files (HTML href/src, CSS url(), JS imports/requests, Python imports),
        mismatched function calls/arguments between modules, incorrect API usage.
        3. Completeness: Check if the code fulfills the core requirements of the original task description. Identify major missing pieces as errors.
        4. Robustness: Basic error handling (missing null checks in JS, unhandled exceptions in Python).
        5. Consistency: Check for glaring inconsistencies (e.g., variable named user_id in one file and userId in another).
        6. ***NEW CHECK***: **File Integrity**: Check if any file appears incomplete or truncated (e.g., ends abruptly mid-function/statement, missing closing tags/brackets/parentheses, contains obvious placeholders like 'TODO', '// Implement later'). Report these as errors requiring fixes.

        OUTPUT FORMAT:
        If you find errors (including integrity/incompleteness errors) in one or more files, output ONLY the complete, corrected content for each file that requires changes.
        Precede each corrected file's content with a marker line exactly like this: --- FIX_START path/to/corrected_file.ext --- (using the original relative path).
        Follow each corrected file's content with a marker line exactly like this: --- FIX_END ---
        DO NOT output content for files that are already correct.
        DO NOT include any explanations, summaries, apologies, or conversational text outside the corrected file blocks.

        If NO errors are found in ANY file after thorough review, output ONLY the exact string: NO_ERRORS_FOUND
        """

        # Step 4: Execute LLM Call
        logging.info(f"[{qa_agent_id}] Sending files to LLM for QA review")

        retry_count = 0
        max_retries = 3
        llm_response = None

        while retry_count < max_retries:
            try:
                # --- Call the generate method ON THE INSTANCE ---
                llm_response = asyncio.run(llm_service_instance.generate( # <-- CHANGE HERE
                    llm_type='gemini', prompt=qa_prompt
                ))
                # --- ---

                if not llm_response.startswith("Error:"):
                    break # Success

                logging.warning(f"[{qa_agent_id}] LLM service returned error: {llm_response}. Retrying ({retry_count+1}/{max_retries})")
                retry_count += 1
                time.sleep(2) # Wait before retry

            except Exception as e:
                logging.error(f"[{qa_agent_id}] Error calling LLM service: {str(e)}")
                retry_count += 1
                time.sleep(2) # Wait before retry

        if retry_count >= max_retries:
             # Raise error if retries exhausted (caught by global handler)
             raise ValueError(f"Failed to get response from LLM after {max_retries} attempts")

        if llm_response is None:
             # Raise error if no response (caught by global handler)
             raise ValueError("No response received from LLM service")


        # Step 5: Parse LLM Response
        corrections_made = False
        corrected_files = {}

        # Log the raw response for reference
        (qa_logs_dir / "llm_raw_response.txt").write_text(llm_response, encoding="utf-8")

        # Check if no errors were found
        if llm_response.strip() == "NO_ERRORS_FOUND":
            logging.info(f"[{qa_agent_id}] No errors found by LLM")
            corrections_made = False
        else:
            # Parse corrected files from response
            try:
                # Find all occurrences of correction blocks
                pattern = r"--- FIX_START\s+(.*?)\s+---\s*(.*?)--- FIX_END ---"
                matches = re.findall(pattern, llm_response, re.DOTALL | re.IGNORECASE) # Added IGNORECASE

                if not matches:
                    logging.warning(f"[{qa_agent_id}] LLM response did not contain any '--- FIX_START ---' blocks. Assuming no corrections intended.")
                    # Log the response for debugging why format wasn't matched
                    (qa_logs_dir / "llm_response_no_fix_blocks.txt").write_text(llm_response, encoding="utf-8")
                    corrections_made = False # Ensure flag is false
                else:
                    for relative_path, corrected_content in matches:
                        relative_path = relative_path.strip()
                        # Normalize path separators for cross-platform compatibility
                        relative_path = relative_path.replace("\\", "/")
                        corrected_content = corrected_content.strip() # Strip leading/trailing whitespace

                        # Validate path (basic check) - ensure it contains a separator or a dot
                        if not relative_path or (not os.path.sep in relative_path and '.' not in relative_path):
                            logging.warning(f"[{qa_agent_id}] Skipping invalid/suspicious path in LLM response: '{relative_path}'")
                            continue

                        corrected_files[relative_path] = corrected_content

                    if corrected_files:
                        corrections_made = True
                        logging.info(f"[{qa_agent_id}] Found {len(corrected_files)} files to correct: {', '.join(corrected_files.keys())}")
                    else:
                         # This case shouldn't be reached if matches were found but all paths invalid
                         logging.warning(f"[{qa_agent_id}] Found FIX_START blocks but no valid paths were extracted.")
                         corrections_made = False


            except Exception as e:
                logging.error(f"[{qa_agent_id}] Error parsing LLM response: {str(e)}")
                (qa_logs_dir / "llm_response_error.txt").write_text(
                    f"Error: {str(e)}\n\nRaw response:\n{llm_response}", encoding="utf-8"
                )
                # Treat as if no corrections were made
                corrections_made = False

        # Step 6: Apply Corrections
        if corrections_made:
            # Log the original versions before applying corrections
            originals_backup_dir = qa_logs_dir / "originals_backup"
            originals_backup_dir.mkdir(exist_ok=True)

            for relative_path, corrected_content in corrected_files.items():
                try:
                    # Normalize path for the target filesystem
                    target_file_path = (developer_files_dir / relative_path).resolve()

                    # Double-check it's still within the agent's files dir (security paranoia)
                    if not str(target_file_path).startswith(str(developer_files_dir.resolve())):
                         logging.error(f"[{qa_agent_id}] Corrected path '{relative_path}' resolved outside agent directory. Skipping.")
                         continue

                    # Backup original if it exists
                    if target_file_path.exists():
                        try:
                            original_content = target_file_path.read_text(encoding="utf-8", errors="replace")
                            # Use Path objects for backup path creation
                            backup_file = originals_backup_dir / relative_path
                            backup_file.parent.mkdir(parents=True, exist_ok=True) # Create parent dirs for backup
                            backup_file.write_text(original_content, encoding="utf-8")
                        except Exception as e:
                            logging.warning(f"[{qa_agent_id}] Could not backup original file {relative_path}: {str(e)}")

                    # Ensure target directory exists
                    target_file_path.parent.mkdir(parents=True, exist_ok=True)

                    # Write corrected content
                    with open(target_file_path, "w", encoding="utf-8") as f:
                        f.write(corrected_content)

                    logging.info(f"[{qa_agent_id}] Applied corrections to: {relative_path}")
                except Exception as e:
                    logging.error(f"[{qa_agent_id}] Error applying corrections to {relative_path}: {str(e)}")
        else:
            logging.info(f"[{qa_agent_id}] No corrections applied")

        # Step 7: Package Final Files (Zip)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"qa_{qa_agent_id}_dev_{developer_agent_id}_{timestamp}.zip"
        zip_filepath = final_zips_dir / zip_filename

        with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Add all files from developer directory
            file_count_in_zip = 0
            if developer_files_dir.exists(): # Check again in case it was deleted mid-process? Unlikely but safe.
                for item in developer_files_dir.rglob('*'):
                    if item.is_file():
                        try:
                            relative_path = item.relative_to(developer_files_dir)
                            zipf.write(item, arcname=str(relative_path))
                            file_count_in_zip += 1
                        except ValueError:
                             logging.warning(f"[{qa_agent_id}] Could not determine relative path for {item}. Skipping in zip.")
                        except Exception as zip_write_err:
                             logging.warning(f"[{qa_agent_id}] Could not add {item.name} to zip: {zip_write_err}")
            else:
                logging.warning(f"[{qa_agent_id}] Developer files directory {developer_files_dir} missing at zipping stage.")


            # Add a QA report file indicating what was done
            report_content = f"""
            QA REPORT
            =========

            Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
            QA Agent: {qa_agent_id}
            Developer Agent: {developer_agent_id}

            Task: {task_title}

            Summary:
            - Text files reviewed: {text_file_count}
            - Binary files skipped: {binary_file_count} ({', '.join(binary_files)})
            - Files corrected by LLM: {len(corrected_files) if corrections_made else 0}
            - Total files packaged: {file_count_in_zip}

            Corrected file paths (if any):
            {(os.linesep + '  ').join(corrected_files.keys()) if corrected_files else '  None'}
            """

            # Add report to zip
            zipf.writestr("qa_report.txt", report_content.strip())

        logging.info(f"[{qa_agent_id}] Created zip file: {zip_filepath}")

        # Step 8: Update QA Agent State
        zip_filepath_str = str(zip_filepath)
        if qa_agent_info_file.exists():
            with open(qa_agent_info_file, "r+") as f:
                qa_agent_data = json.load(f)
                qa_agent_data["state"] = "finished_qa_work"
                qa_agent_data["output_zip_path"] = zip_filepath_str
                qa_agent_data["assigned_developer_id"] = None
                qa_agent_data["corrections_made"] = corrections_made
                qa_agent_data["corrected_files"] = list(corrected_files.keys()) if corrected_files else []
                f.seek(0)
                json.dump(qa_agent_data, f, indent=2)
                f.truncate()
            logging.info(f"[{qa_agent_id}] Updated agent state to 'finished_qa_work'")
        else:
            logging.warning(f"[{qa_agent_id}] Agent info file not found: {qa_agent_info_file}")

        logging.info(f"[{qa_agent_id}] QA workflow completed successfully")
        return zip_filepath_str

    except Exception as e:
        # Global error handling
        error_traceback = traceback.format_exc()
        logging.error(f"[{qa_agent_id}] Critical error during QA work: {str(e)}\n{error_traceback}")

        # Create error report
        error_report = f"""
        CRITICAL ERROR DURING QA PROCESS
        ===============================

        QA Agent: {qa_agent_id}
        Developer Agent: {developer_agent_id}
        Time: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

        Error: {str(e)}

        Traceback:
        {error_traceback}
        """

        error_file_path = qa_logs_dir / f"critical_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        try:
            with open(error_file_path, "w", encoding="utf-8") as f:
                 f.write(error_report)
        except Exception as log_err:
            logging.error(f"[{qa_agent_id}] Additionally failed to write critical error log: {log_err}")


        # Create error zip
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"qa_{qa_agent_id}_dev_{developer_agent_id}_{timestamp}_error.zip"
        zip_filepath = final_zips_dir / zip_filename

        try:
            with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:
                if error_file_path.exists(): # Check if error log was written
                    zipf.write(error_file_path, arcname="critical_error.txt")

                # Add any developer files that were loaded before the error occurred
                if 'project_files' in locals() and project_files and developer_files_dir.exists():
                    for filepath_str in project_files.keys():
                        source_file = developer_files_dir / filepath_str
                        if source_file.exists() and source_file.is_file(): # Check if it's a file
                            try:
                                zipf.write(source_file, arcname=filepath_str)
                            except Exception as zip_err:
                                logging.warning(f"[{qa_agent_id}] Could not add {filepath_str} to error zip: {str(zip_err)}")
        except Exception as zip_error:
            logging.error(f"[{qa_agent_id}] Failed to create error zip: {str(zip_error)}")

        # Update QA agent state
        zip_filepath_str = str(zip_filepath)
        if qa_agent_info_file.exists():
            try:
                with open(qa_agent_info_file, "r+") as f:
                    qa_agent_data = json.load(f)
                    qa_agent_data["state"] = "error"
                    qa_agent_data["error_details"] = str(e)
                    qa_agent_data["output_zip_path"] = zip_filepath_str
                    qa_agent_data["assigned_developer_id"] = developer_agent_id # Keep reference
                    f.seek(0)
                    json.dump(qa_agent_data, f, indent=2)
                    f.truncate()
            except Exception as e_update:
                logging.error(f"[{qa_agent_id}] Failed to update agent state to error: {str(e_update)}")

        return zip_filepath_str # Return path to error zip


# qa_agent.py

# import json
# import logging
# import asyncio
# import zipfile
# import re
# from pathlib import Path
# from datetime import datetime
# import traceback
# import os
# import time
# import subprocess

# from llm_service import LLMService

# # Precompile regex for diff blocks
# DIFF_BLOCK_RE = re.compile(
#     r"--- DIFF_START\s+(.*?)\s+---\s*(.*?)--- DIFF_END ---",
#     re.DOTALL | re.IGNORECASE
# )


# def perform_qa_work(qa_agent_id, qa_agent_dir, developer_agent_id, developer_agent_dir_path_str):
#     """
#     Performs QA review on files generated by a developer agent using diff-and-patch.
#     Returns: Path to the final zip file containing reviewed files
#     """
#     developer_agent_dir = Path(developer_agent_dir_path_str)
#     qa_logs_dir = qa_agent_dir / "logs"
#     developer_files_dir = developer_agent_dir / "files"
#     developer_task_file = developer_agent_dir / "task.json"
#     qa_agent_info_file = qa_agent_dir / "agent_info.json"
#     final_zips_dir = Path("output") / "final_zips"
#     qa_logs_dir.mkdir(parents=True, exist_ok=True)
#     final_zips_dir.mkdir(parents=True, exist_ok=True)

#     llm = LLMService()
#     logging.info(f"[{qa_agent_id}] Starting QA work for Developer {developer_agent_id}")

#     try:
#         # Step 1: Load project files into memory
#         project_files = {}
#         binary_files = []
#         for item in developer_files_dir.rglob('*'):
#             if item.is_file():
#                 rel = item.relative_to(developer_files_dir)
#                 if item.suffix.lower() in {'.jpg','.png','.gif','.bin'}:
#                     binary_files.append(str(rel))
#                 else:
#                     project_files[str(rel)] = item.read_text(errors='replace')
        
#         # Step 2: Load original task
#         task_title = task_description = None
#         if developer_task_file.exists():
#             data = json.loads(developer_task_file.read_text())
#             task_title = data.get('title')
#             task_description = data.get('description')
#         task_title = task_title or 'Unknown Task'
#         task_description = task_description or ''

#         # Step 3: Build prompt
#         combined = ''
#         for path, content in project_files.items():
#             combined += f"--- START {path} ---\n{content}\n--- END {path} ---\n\n"
#         binary_info = ''
#         if binary_files:
#             binary_info = "Binary files skipped:\n" + "\n".join(binary_files)

#         qa_prompt = f"""
# You are a QA engineer. Identify missing imports, syntax errors, and integration issues, and return only unified diffs.

# Task: {task_title}
# Description: {task_description}

# {combined}
# {binary_info}

# For each file needing fixes, output a diff in this format:
# --- DIFF_START path/to/file.ext ---
# <unified diff>
# --- DIFF_END ---
# If no changes needed, output only: NO_CHANGES_NEEDED
# """

#         # Step 4: Call LLM
#         response = None
#         for attempt in range(3):
#             response = asyncio.run(llm.generate(llm_type='gemini', prompt=qa_prompt))
#             if not response.startswith('Error:'):
#                 break
#             logging.warning(f"[{qa_agent_id}] LLM error, retry {attempt+1}")
#             time.sleep(2)
#         if response is None or response.startswith('Error:'):
#             raise RuntimeError('LLM failed to return a valid response')

#         (qa_logs_dir / 'llm_response.txt').write_text(response)
        
#         # Step 5: Apply diffs
#         if response.strip() == 'NO_CHANGES_NEEDED':
#             logging.info(f"[{qa_agent_id}] No changes needed")
#         else:
#             for file_path, diff_text in DIFF_BLOCK_RE.findall(response):
#                 target = developer_files_dir / file_path.strip()
#                 if not target.exists():
#                     logging.warning(f"File to patch not found: {file_path}")
#                     continue
#                 # write diff to temp file
#                 diff_file = qa_logs_dir / f"patch_{file_path.replace('/','_')}.diff"
#                 diff_file.write_text(diff_text)
#                 # apply patch
#                 try:
#                     subprocess.run(
#                         ['patch', str(target), '--input', str(diff_file)],
#                         check=True, capture_output=True, text=True
#                     )
#                     logging.info(f"Applied patch to {file_path}")
#                 except subprocess.CalledProcessError as e:
#                     logging.error(f"Failed to apply patch to {file_path}: {e.stderr}")

#         # Step 6: Zip results
#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#         zip_name = f"qa_{qa_agent_id}_dev_{developer_agent_id}_{timestamp}.zip"
#         zip_path = final_zips_dir / zip_name
#         with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
#             for item in developer_files_dir.rglob('*'):
#                 if item.is_file():
#                     zf.write(item, arcname=item.relative_to(developer_files_dir))
#             # include report
#             report = f"QA Complete for {developer_agent_id} at {timestamp}\n"
#             zf.writestr('qa_report.txt', report)

#         # Step 7: Update agent state
#         if qa_agent_info_file.exists():
#             data = json.loads(qa_agent_info_file.read_text())
#             data.update({
#                 'state': 'finished_qa_work',
#                 'output_zip_path': str(zip_path)
#             })
#             qa_agent_info_file.write_text(json.dumps(data, indent=2))

#         logging.info(f"[{qa_agent_id}] QA workflow done")
#         return str(zip_path)

#     except Exception as e:
#         logging.error(f"[{qa_agent_id}] Critical error: {e}\n{traceback.format_exc()}")
#         # handle error similar to before...
#         # omitted for brevity
#         raise
